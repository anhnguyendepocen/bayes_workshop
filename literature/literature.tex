\documentclass[12pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[american]{babel}
\usepackage{tikz}
\usepackage{csquotes}
\usepackage{amsmath}

\usepackage{csquotes}
\usepackage{color}
\usepackage{attrib}
\usepackage{enumitem}
\usepackage{blindtext}
\usepackage[hidelinks]{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black, % references (e.g. pages)
	linkcolor=black, %table of content links etc.
	urlcolor=blue
}

\usepackage[backend=biber, style=apa]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{../bibliography.bib}


\title{Commented Literature List \\ (Draft)}
\author{Fabian Dablander}
\date{\today}

\begin{document}
\maketitle

\section{General Linear Model}
\begin{enumerate}[label=-]
  \item \fullcite{cronbach1957two}
  \item \fullcite{cohen1968multiple}
  
  \item \fullcite{kery2010introduction}
  
  \textbf{The first four chapters are arguably the best introduction to the General Linear Model that there is. Featuring R code - most prominently the
  \emph{model.matrix} function - the author introduces the t-test, ANOVA, multiple regression etc. as being basically the same thing. Highly recommended.}
  
  
  \item \fullcite{poline2012general}
  
  \textbf{This is a hilarious and well written paper. It gives an easy to understand introduction to the GLM using simple notation, and offers some psychological investigations on why neuroimaging people love it (one can easily incorporate a lot of different things, parameters for low frequency drift, motion correction etc.) The appendix derives $\hat{\beta}$ as the ordinary least square solution, which in the case of the GLM is the best linear unbiased estimate.}
  \item \fullcite{monti2011statistical}
  
  \textbf{This paper discusses the problems of the ''Massive Univariate Approach`` in-depth, ranging from heteroscedasticity to violations of additivity in modeling the HRF-response. Single subject analysis as well as multiple subject analysis (using the summary statistics approach) are discussed. In general, this paper shows how a research community's creativity is boosted when a beloved tool - the General Linear Model - gets into (deep) trouble.}
\end{enumerate}

\section{Hierarchical Modeling}
\begin{enumerate}[label=-]
    \item \fullcite{efron1977stein}
    \item \fullcite{baayen2008mixed}
    \item \fullcite{barr2013random}
    \item \fullcite{bates2015parsimonious}
    \item \fullcite{rouder2007signal}
    \item \fullcite{rouder2005introduction}
    \item \fullcite{rouder2013hierarchical}
\end{enumerate}



\section{Deficits of Frequentist Statistics}

\begin{displayquote}
\textbf{``The textbooks are wrong. The teaching is wrong. The seminar you just attended is wrong. The most prestigious journal in your scientific field is wrong.''} - Ziliak and McCloskey (2008)
\end{displayquote}

\begin{description}
  \item \fullcite{bakan1966test}
  
  \item \fullcite{royall1986effect}
  
  \item \fullcite{berger1988statistical}
   
  \item \fullcite{cohen1994earth}
  
  \item \fullcite{cohen1990things}
  
  \item \fullcite{loftus1996psychology}
  
  \item \fullcite{nickerson2000null}
  
  \item \fullcite{berger1987testing}
  
  \item \fullcite{sellke2001calibration}
  
  \item \fullcite{haller2002misinterpretations}
  
  \item \fullcite{gigerenzer2004mindless}
  
  \textbf{This is required reading.}
  
  \item \fullcite{lee2005bayesian}
    
  \item \fullcite{wagenmakers2007practical}
  
  \textbf{In my opinion the best exposition of the deficits of null hypothesis significance testing. Uses extremely illustrative examples to show that p values depend on data never observed (sampling distribution), on the subjective intentions of researchers (binomial vs. negative binomial), can lead to highly inflated Type I error rates due to optional stopping (see also \cite{simmons2011false}), do not quantify statistical evidence, and differ in their interpretation in small versus large sample sizes (the p-postulate is false).}
  
  \textbf{Also introduces Bayesian estimation and hypothesis testing on a coin toss example. Goes on to suggest the Bayesian Information Criterion (BIC) as simple alternative to significance testing.}
  
  \item \fullcite{wagenmakers2008bayesian}
  
  \textbf{Is similar to \cite{wagenmakers2007practical} in that it discusses the disadvantages of null hypothesis significance testing. Additionally notes that the p-value cannot be used to compare non-nested models, and that it relies on ad-hoc principles since it does not have as solid a foundation as Bayesian inference (probability theory) and thus does not specify a unique solution to every statistical problem. Discusses the advantages of Bayesian inference, i.e. coherence, automatic parsimony, extension to non-nested models, no optional stopping, possibility to quantify evidence in favour of the null etc. Features a hilarious anecdote about the subjective nature of the \emph{p} value.}
  
  \item \fullcite{johansson2011hail}
  
  \textbf{This is essentially a recap of two books: Royall (1997) and Dienes (2008). After discussing Fisher's and Neyman's stance on significance testing and their deficits, introduces likelihoods as a measure to quantify statistical evidence. Bayesian inference extends the likelihood paradigm by assigning prior distributions over parameters to allow composite hypotheses. All in all, this is an excellent paper.}
  
  \item \fullcite{hoekstra2014robust}
  
  \item \fullcite{morey2015fallacy}
  
  \item \fullcite{rouder2015lunch}
    
  \item \fullcite{hoenig2001abuse}
  
  \item \fullcite{wagenmakers2014power}
  
  \textbf{Power is a pre-experimental concept that averages over all possible outcomes of an experiment, only one of which is actually observed. Thus low powered experiment can lead highly valuable information, while high powered experiments might be uninformative. As Ioannidis \cite{ioannidis2005most} and others \cite{button2013power} have shown, low power effects the likelihood that a significant finding actually reflects a true effect; because using Bayes' theorem}
  
  \begin{equation*}
    \frac{P(H_1 | p < \alpha)}{P(H_0 | p < \alpha)} = \frac{P(p < \alpha | H_1)}{P(p < \alpha | H_0)} \times \frac{P(H_1)}{P(H_0)}
  \end{equation*}
  
  \textbf{where $P(p < \alpha | H_1)$ is $1 - \beta$ and $P(p < \alpha | H_0)$ is $\alpha$. Thus $\frac{1 - \beta}{\alpha}$ is the extent to which the observation $p < \alpha$ changes the prior odds that $H_1$ rather than $H_0$ is true.}
  
  \textbf{However, when the actual data is observed, we can go beyond measures of diagnosticity such as power. Assume two urns, $H_0$ with nine green balls and one blue ball, $H_1$ with nine green balls and one orange ball. Your task is decide which urn you draw from (the true urn is $H_1$). In one experiment, you only draw on ball - yielding statistical power of 0.10. However, you were lucky and drew an orange ball, which means that the urn is $H_1$. Although low powered, your experiment yielded decisive support. Conversely, you could have drawn 22 balls, yielding statistical power of 0.90, all of which where green. Despite high power, your experiment would have been uninformative. The paper features another example using a t-test; go read it!}
\end{description}

\section{Bayesian Statistics}

\subsection{Motivation}
\begin{description}  
  \item \fullcite{edwards1963bayesian}
  
  \textbf{This is the earlierst exposition of Bayesian ideas specifically written for psychologists. It is quite extensive, in-depth and gives a broad overview about Bayesian merits. It is considered a classic.}
  
  \item \fullcite{raftery1995bayesian}
  
  \item \fullcite{kass1995bayes}
  
  \item \fullcite{wagenmakers2011psychologists}
  
  \item \fullcite{dienes2011bayesian}
  
  \textbf{Clarifies the differences between frequentist and Bayesian statistics with respect to optional stopping, planned versus post hoc comparisons and multiple testing. Discusses the likelihood principle and how classical statistics violates it. Argues that if you want to be rational (which scientist would not?) you should be using Bayesian inference.
  }
  
  \item \fullcite{rouder2013hierarchical}
  
  \item \fullcite{wagenmakers2014need}
  
  \item \fullcite{dienes2015bayes}
\end{description}


\subsection{Priors}
\begin{description}
  \item \fullcite{lindley1957statistical}
  
  \textbf{The ''Jeffreys-Lindley`` paradox is an intriguing finding. Assume a normal model $N(\theta, \sigma^2)$ with known variance $\sigma^2$, $H_0: \theta = \theta_0$ and $H_1: \theta \neq \theta_0$. For many prior distributions over $\theta$ and any $\alpha \in [0, 1]$ we can find a sample size $N$ such that}
  
  \begin{itemize}
    \item the sample mean $\hat x$ is significantly different from $\theta_0$ at level $\alpha$
    \item the posterior probability that $\theta = \theta_0$ is at least as big as $1 - \alpha$
  \end{itemize}
  
  \textbf{see also \cite{sprenger2013testing} and \cite{robert2014jeffreys}. Stated differently, while the p-value would lead to the rejection of $H_0$, the Bayes factor- under certain diffuse prior distributions - would lead to acceptance of $H_0$. It also shows that as the prior on $\theta$ gets more diffuse, the Bayes factor favours $H_0$ without bound. It is important to note that the p-value does not look at the data under $H_1$, while the Bayes factor quantifies evidence under $H_0$ and $H_1$ - the marginal likelihoods (this is one reason why in frequentist statistics one cannot accept $H_0$).
  }
  
  \item \fullcite{degroot1982lindley}
  
  \textbf{Short take on Lindley's paradox demonstrating that one should not use diffuse priors when testing hypothesis, since they favour the null; this is because by spreading out their prior, the marginal likelihood decreases.}
 
  \item \fullcite{liu2008bayes}
  
  \textbf{Shows on a simple example how the Bayes factor is influenced by the prior. The marginal likelihood, i.e. the probability of the data given some Model $M$ is $p(D|M) = \int p(D|\theta, M)p(\theta| M) \mathrm{d}\theta$ where $p(D|\theta, M)$ is the likelihood and $p(\theta|M)$ is the prior over the parameter vector $\theta$. Stated differently, the marginal likelihood is the sum of the likelihood evaluated at each value of the model parameters weighted by the prior over the parameters. Parameter estimation is not heavily influenced by prior assumptions. Other methods of model comparison such as the \emph{posterior predictive loss}, the \emph{Deviance information criterion} and the \emph{posterior likelihood ratio} are discussed, all of which capitalize on the robustness of the posterior relative to the prior. When using the Bayes factor, make sure to run a sensitivity analysis; that is, see if the conclusions change when using a variety of different priors.}
  
  \item \fullcite{vanpaemel2010prior}
  
  \textbf{Argues that the marginal likelihood - and thus the Bayes factor - is an appropriate measure for evaluating psychological models precisely because of its sensitivity to the prior. Priors instantiate psychological theory and restrict the parameter values in the formal model; they are thus the antidote to the \emph{Greek letter syndrome}, which leads sufferers to not consider the meaning of the parameters and ignore the underlying theory they instantiate. Concludes in arguing that one does not need sensitivity analysis, since the prior instantiates theory (it is not an arbitrary assumption). Contrary to \cite{liu2008bayes}, users of posterior measures should use sensitivity analysis. Note that the paper talks about evaluation of formal mathematical models that instantiate theories, not models used for data analysis like regression.}
\end{description}


\subsection{Bayes Factor}
\begin{description}
  \item \fullcite{rouder2009bayesian}
  
  \item \fullcite{wetzels2009quantify}
  
  \item \fullcite{wagenmakers2010bayesian}
  
  \textbf{In general, Bayes factors, that is marginal likelihoods, are hard to compute. An important mathematical result, the Savage-Dickey density ratio, allows one to calculate the Bayes factor in one go:}
  
  \begin{equation*}
    BF_{01} = \frac{p(D|H_0)}{p(D|H_1)} = \frac{p(\Phi = \Phi_0|D, H_1)}{p(\Phi = \Phi_0|H_1)}
  \end{equation*}
  
  \textbf{This only works for nested models (a constraint also for the p-value), where $H_0$ posits that $\Phi = \Phi_0$, i.e. some substantive parameter is zero. Graphically, Savage-Dickey is the ratio between the height of the posterior and the height of the prior at the parameter value of interest. One clearly sees how the prior affects the Bayes factor, see also \cite{liu2008bayes}, and that \emph{nuisance parameters} irrelevant - they are present in both models and cancel out. The paper reviews much of Bayesian inference on a binomial example and features two real world examples applying Savage-Dickey. A must read; but see \cite{morey2011using} for a better computational approach.}
  
  \item \fullcite{rouder2012regression}
  
  \textbf{Enlightening paper. Frequentist statistics cannot support $H_0$ - absence of evidence is not evidence of absence - since significance testing is asymmetric; alternative models are not specified; free lunch problem, see also \cite{rouder2015lunch}. This leads to the p-value being biased against $H_0$, see also \cite{wagenmakers2007practical, rouder2009bayesian}. The Bayes factor is introduced via example of linear regression.}
  
  \textbf{Instead of only allowing for point alternative hypotheses - like likelihood-ratio tests do -, Bayes factor allows for interval alternatives; the alternative becomes the average likelihood of the point alternatives weighted by the prior. By reparameterizing the model in terms of standardized effect size with $\beta \sim Cauchy(s)$ the Bayes factor becomes invariant to transformations. Additional beneficial properties include \emph{Consistency}, i.e. if $M_1$ then $B_{10} \sim \inf$; conversely if $M_0$ then $B_{10} \sim 0$. Note that p-values are not consistent, since they are randomly distributed under $H_0$; and \emph{Consistency in information}, i.e. data should affect the Bayes factor only through $R^2$.}
  
  \textbf{Generalizes the default prior to multiple regression. Cauchy is computationally hard, so a multivariate normal prior is used for the slope parameters, with an inverse-gamma specified over the variance of the standardizes slope. After a short example, the authors discuss three concerns: \emph{the null model is never true}, \emph{all models are wrong} and the \emph{subjective nature} of Bayes.}
  
  \item \fullcite{rouder2012anova}
  
  \item \fullcite{wetzels2012default}
  
  \item \fullcite{ly2014jeffreys}
  
  \textbf{Introduces the ''default`` or ''objective`` priors developed by Harrold Jeffreys to have certain desirable properties. Introduces them by means of the t-test and the Pearson correlation. Very in-depth, not light on mathematics.}
\end{description}


\subsection{Parameter Estimation}
\begin{description}
  \item \fullcite{kruschke2013bayesian}
  
  \item \fullcite{wagenmakers2014paradox}
  
  \textbf{This is an important paper demonstrating that parameter estimation and hypothesis testing are not (always) equivalent. While we learn something about the parameter with every observation we make, the observation need not be helpful in choosing between different hypotheses or models. Hypothesis testing via parameter estimation (i.e. saying that the interval excludes zero) is inappropriate. They answer different questions.}
\end{description}

\subsection{Posterior Predictive Checks}
\begin{description}
    \item \fullcite{gelman2003bayesian}
    \item \fullcite{gelman2013understanding}
    \item \fullcite{sinharay2006posterior}
\end{description}


\subsection{Different strands of Bayesian statistics}
\begin{description}
  \item \fullcite{berger2006case}
  \item \fullcite{gelman2013philosophy}
\end{description}

\subsection{Model Comparison}
\begin{description}
  \item \fullcite{vandekerckhove2014model}
  
  \textbf{Bayes factors, unlike p-values, are not limited to nested models. Like the AIC and BIC they can be used to compare completely different theoretical models. In choosing between models, one has to make a trade-off between goodness of fit, i.e. how well the model explains the data at hand, and parsimony. Highly complex models will have a higher goodness of fit - because the also fit noise. Commonly employed methods to account for model complexity are the AIC and BIC. However, since they only count the parameters of the models, they do not account for their respective functional form. Bayes factors to exactly that. Additionally, they employ an automatic Ockham's razor, since complex models spread out their predictions, decreasing the marginal likelihood. Read the paper!}
\end{description}

\subsection{Replication}
\begin{description}

  \item \fullcite{simonsohn2015small}
  
  \textbf{Only here because I don't like it. And to show the contrast to the paper down below - which I prefer much more.}

  \item \fullcite{verhagen2014bayesian}

\end{description}


\subsection{Applications}
\begin{description}
  \item \fullcite{rouder2011bayes}
  
  \item \fullcite{morey2011bayes}
 
  \item \fullcite{wetzels2011statistical}
  
  \item \fullcite{dienes2014using}

  \item \fullcite{wagenmakers2015absence}
  
  \textbf{Builds on work by \cite{donnellan2014association} who in nine experiments with over 3.000 participants in total failed to replicate \cite{bargh2012bath}. Since one cannot within the framework of null hypothesis significance testing support the null hypothesis, Wagenmakers and colleagues use Bayesian inference to do just that. Also excellent exposition on how high powered experiments do not necessarily yield high evidential value, see also \cite{wagenmakers2014power}. Three studies consisting of a sample size of 210, 494 and 553 participants were uninformative!}

  \item \fullcite{nieuwenstein2015making}
\end{description}


\subsection{Problems?}
\begin{description}
  \item \fullcite{sanborn2014frequentist}
  
  \item \fullcite{rouder2014optional}
\end{description}




\section{Software}
\begin{description}
  \item \fullcite{stan}
  
  \item \fullcite{bayesfactor}
  
  \textbf{This is an awesome R package written by Richard Morey. It does all the GLM things and contingency tables. Generalized Linear Models such as logistic regressions have yet to be implemented. Checkout the excellent \href{http://bayesfactorpcl.r-forge.r-project.org/}{documentation} which features many examples.}
  
  \item \fullcite{jasp}
  
  \textbf{JASP is a low fat alternative to SPSS, and a Bayesian trojan horse. It is easy and intuitive to use. Checkout the \href{https://jasp-stats.org/}{homepage} which also includes materials from workshops.}
\end{description}




\section{Statistical Power}

\begin{description}
  \item \fullcite{cohen1962statistical}
  
  \item \fullcite{sedlmeier1989studies}
  
  \item \fullcite{maxwell2004persistence}
  
  \item \fullcite{cumming2001primer}
  
  \item \fullcite{button2013power}
  
  \item \fullcite{fraley2014n}
  
  \item \fullcite{yarkoni2009big}
  
  \item \fullcite{ioannidis2005most}
  
    \textbf{Very famous paper arguing how publication bias, false incentives and low power lead to most published research being false. Introduces a simple formula for the posterior predictive value of studies $PPV = \frac{(1 - \beta)R}{(1 - \beta)R + \alpha}$, i.e. the probability of a research finding being true, where $R$ are the pre-study odds of the hypothesis being true, $\beta$ is the Type II error rate, and $\alpha$ is the Type I error rate. For further reading on how low power undermines evidential values of studies, see also \cite{button2013power}}
    
  \item \fullcite{gelman2014beyond}
\end{description}


\section{Miscellaneous}

\subsection{p-hacking et al.}
\begin{description}
  \item \fullcite{simmons2011false}
  
  \textbf{Introduces the term ''p-hacking``, that is exploiting one's freedom in data collection, statistical analysis and reporting in order to increase the chance of a significant finding (which then will most likely turn out to be a false positive). Demonstrates that basically anything can be presented as significant, using an experimental study and simulations.}
  
  \item \fullcite{john2012measuring}
  
  \textbf{Asking over 2.000 psychologists, report the prevalence of ''questionable research practices'' such as ''failing to report all dependent measures in a study`` ($\sim 64\%$), ''optional stopping`` ($\sim 56\%$), ''harking`` ($\sim 31\%$) etc.}
  
  \item \fullcite{gelman2013garden}
  
  \textbf{Beautiful paper arguing that ''p-hacking`` can be an entirely unconscious process where researchers have only the best intentions in mind! Draws on examples from the literature to discuss a hidden multiple comparison problem - the garden of forking paths. Psychological studies, although presented as confirmatory, are frequently exploratory in nature.}
  
  \item \fullcite{bakker2012rules}
  
  \textbf{In the game of psychological science, you either reach significance, or you die.}
\end{description}


\subsection{Historical Notes on Modern Statistics}
\begin{description}
  \item \fullcite{lindley2004bayesian}
  
  \item \fullcite{gigerenzer1993superego}
  
  \textbf{Excellent, must read paper about the ``inference revolution''. Using a Freudian analogy, describes R.A. Fisher as the Ego, specifying no alternative hypothesis, having no fixed $\alpha$-level and interpreting the p-value as strength of evidence. The Neyman-Pearson approach to statistical testing is described as the Superego, including an alternative hypothesis, specifying a pre-fixed $\alpha$ and $\beta$-level and interpreting the p-value as either being above the treshold or below - not as strength of evidence, but as decision to act: either reject the null hypothesis, or stay in limbo. Finally, the Freudian Id is Bayes, giving us what we really want: the probability of the hypothesis given the data $P(H|D)$. The p-value \emph{cannot} achieve this; it merely is the probability of the data given data as extreme or more extreme as the once observed, $P(D|H)$. Incredibly fun read.}
  
  \item \fullcite{berger2003could}
  
  \item \fullcite{gigerenzer2015surrogate}
  
  \textbf{Recent exposition of statistical inference in general, and the problems with dogma. Also discusses Bayesian statistics.}
\end{description}

\end{document}