---
title: "Bayesian Statistics"
author: "Fabian Dablander, Peter Edelsbrunner"
bibliography: "../bibliography.bib"
csl: "../apa.csl"
output:
  ioslides_presentation:
    widescreen: yes
    incremental: true
    css: styles.css
---

## Outline <!-- Fabian -->

- Statistics: The Status Quo
    * 1.1 A Bird's Eye View
    * 1.2 An Example
    * 1.3 Notes on the history of statistics
    
- frequentist inference, bootstrapped
<!-- was not in the original workshop (would have taken too long)
     I've put it here for people to look through it, if they wanted to
-->


- p-values: A Death in Five Acts
    * 2.1 Sampling: Hidden subjectivity
    * 2.2 Sampling: When to stop?
    * 2.3 Inference: Statistical Evidence
    * 2.4 Inference: There is no free lunch
    * 2.5 Inference: Answer the wrong question
    
## Outline
    
- Confidence Intervals and Effect size: A Frequentist solution?
    
- Likelihoods, with demo
    
- Elegant extension: Bayesian Inference
    * 4.1 Prior distributions
    * 4.2 Bayes factors, with demo
    * 4.3 Credibility Intervals

## Outline

- Practical Bayes
    * 6.1 JASP
    * 6.2 turning the hands of time
    * 6.3 loneliness and bathing habits
    * 6.4 do women talk more than men?
    * 6.5 testes size and caregiving

- Where to go from here?

## note
- if you have internet, please install http://jasp-stats.org
- let's do it!

# The Status Quo in Statistics

## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     .
.                          .
.                          .
.                          .
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>



## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
.                          .
.                          .
.                          .
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>




## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 .
.                          .
.                          .
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>




## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
.                          .
.                          .
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>



## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                .
.                          .
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>




## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
.                          .
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 .
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 Rational
.                          .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 Rational
Ugly                       .
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 Rational
Ugly                       Pretty
.                          .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 Rational
Ugly                       Pretty
Irrelevant                 .
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 Rational
Ugly                       Pretty
Irrelevant                 Relevant
.                          .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 Rational
Ugly                       Pretty
Irrelevant                 Relevant
what's taught              .

<div id="contrast">borrowed from EJ Wagenmakers</div>


## A Bird's Eye View <!-- Fabian -->
  Classical Statistics     Bayesian Statistics
----------------------     ---------------------------
Ad-hoc                     Axiomatic
Incoherent                 Coherent
Paradoxical                Intuitive
Irrational                 Rational
Ugly                       Pretty
Irrelevant                 Relevant
what's taught              what's not taught

<div id="contrast">borrowed from [EJ Wagenmakers](https://docs.google.com/file/d/0B-Ww24m3ZkEyMEpudlVsX3pRVzA/edit)</div>


## put succinctly ...
> "The textbooks are wrong. The teaching is wrong. The seminar
> you just attended is wrong. The most prestigious journal in your
> scientific field is wrong.”

> - Ziliak and McCloskey (2008)

## An Example: Risk Behaviour and Social Status <!-- Peter -->

<!--  Show example in R: Research Question:
      Is Risk Behaviour related to Popularity (how popular classs peers rate you)
      and to Likeability (how much class peers like you)
      -> Discuss: What does the p-value tell us?
      -> Is there something strange about it? Yes?
      - in that case, let's explore why we use this stuff!
-->

<div align="center">
<img height='500' src='img/mult_risk.png'>
</div>

## An Example: Risk Behaviour and Social Status <!-- Peter -->

```{r, eval=FALSE}
risk_bayes <- read.csv("riskdata_bayes.csv") # Load data
summary(lm(risksum ~ ZPop + ZLike, data = risk_bayes)) # Multiple regression
```

<div align="center">
<img height="300" src="img/Rlinreg.png">
</div>

## An Example: Risk Behaviour and Social Status <!-- Peter -->

- What does *p* tell us?

- *The probability to get data this or more extreme in case that in the underlying population there is no association between popularity/likeability and risk behaviour is less than 0.1%*

- Does this sound *strange* to you?


## history: how did we get here? <!-- Fabian -->
- quick question: how old do you think is **modern statistics**?

## Ronald A. Fisher
<img src="img/fisher.jpg" width=500 height=500 />


## Ronald A. Fisher
- **Statistical Methods for Research Workers** (1925)
- **Design of Experiments** (1935)
- only talked about $H_0$ and $\alpha$; no alternative hypothesis! no power!
- p-value indicates **strength of evidence**!
- that is, $p = 0.001$ is **better** than $p = 0.049$


## Ronald A. Fisher
> "No scientific worker has a fixed level of significance at which from year to year, and in all
> circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light
> of his evidence and his ideas."

- R.A. Fisher (1956), as cited in [@gigerenzer2014surrogate, p.1]

## Jerzy Neyman and Egon Pearson
- extended Fisher's work, but **changed it substantially**
- introduced the alternative hypothesis, $H_1$
- concept of a $\beta$ error; statistical power
- $p < \alpha$, or $p > \alpha$
- binary cut-offs; not **statistical evidence**!
- they did **behavioral statistics**

## Neyman-Pearson
> "We are inclined to think that as far
> as a particular hypothesis is concerned, no test based upon the theory
> of probability can by itself provide any valuable evidence of
> the truth or falsehood of that hypothesis.


## Neyman-Pearson
> "We are inclined to think that as far
> as a particular hypothesis is concerned, no test based upon the theory
> of probability can by itself provide any valuable evidence of
> the truth or falsehood of that hypothesis.

> But we may look at the
> purpose of tests from another view-point. **Without hoping to know**
> **whether each separate hypothesis is true or false**, we may search
> for rules to **govern our behaviour** with regard to them, in following
> which we insure that, in the long run of experience, we shall not be too often wrong."

- Neyman and Pearson (1933), as cited in [@johansson2011hail, p.118]

## unification / bastardization
- in the 40s and 50s **psychologists** (i.e. Guilford) wrote popular statistics books
- they **did not** distinguish between Fisher's and Neyman-Pearson's approach
- current statistical practice is a hybrid between those two **incompatible** paradigms

## unification / bastardization
> statisticians "have already overrun every branch of science with a rapidity of conquest
> rivalled only by Attila, Mohammed, and the Colorado beetle."

- did Piaget ever compute a p-value?
- did Skinner? what's with Köhler? Pavlov?

## mindless statistics
- 1a: setup a statistical hypothesis of **no difference** or **zero correlation**
- 1b: don't specify the predictions of your research hypothesis
- 2: Use 0.05 as a convention for rejecting the null
- 3: always use this procedure [@gigerenzer2004mindless]


## what is probability? <!-- Fabian -->
- for frequentists, probability is the long-run relative frequency of events
- for example, the probability of a coin coming up heads is the proportion of heads in an **infinite** amount of tosses
- to ask about the probability of the next coin toss being heads is **nonsensical**
- the next coin toss is either heads, or it is not
- single events can't be assigned probability!
- **repeatability** becomes a crucial ingredient
- **Fisher**: geneticist, worked in agriculture, did lots of repeatable experiments
- **Jeffreys**: geophysicist, studied earthquakes etc., no way of repeating experiments

## what is probability?
- frequentists cannot talk about
 * the probability of a 3rd world war
 * climate change
 * you failing your next exam
 * any non repeatable event
- these issues are about **uncertainty**, and need a Bayesian take on probability
- by conceptualising probability as long-run average frequency, classical statistics deprived itself of answering the essential question in science
- ***what is the probability that my hypothesis / theory is true?***

# what is in a p-value? <!-- Fabian -->

## some statements
- suppose you run an experiment testing the effect of some treatment
- you have a control group (n = 20) and a treatment group (n = 20)
- you run an independent t-test and get $t(18) = 2.7, p = 0.01$
- let's rate some statements!
- from @haller2002misinterpretations, who got it from @oakes1986statistical

## statement 1
- You have absolutely disproved the null hypothesis of no difference.

## statement 1
<img src="img/hell_NO.jpg" width=400 height=400 />

## statement 1
- You have absolutely disproved the null hypothesis of no difference.
- everything in statistics is probabilistic, there are no **absolutes**

## statement 2
- You have found the probability of the null hypothesis being true.

## statement 2
<img src="img/cat_NO.jpg" width=400 height=400 />

## statement 2
- You have found the probability of the null hypothesis being true.
- in classical statistics, you cannot assign probabilities to your hypotheses
- but every researcher really, really, really, really wants that!
- one reason for Bayesian statistics ;)

## statement 3
- You have absolutely proved your experimental hypothesis (there is a difference).

## statement 3
<img src="img/hell_NO.jpg" width=400 height=400 />

## statement 3
- You have absolutely proved your experimental hypothesis (there is a difference).
- same as statement 1: probabilistic, not absolute!

## statement 4
- You can deduce the probability of the experimental hypothesis being true.

## statement 4
<img src="img/cat_NO.jpg" width=400 height=400 />

## statement 4
- You can deduce the probability of the experimental hypothesis being true.
- same as statement 2: can't assign probabilities to hypotheses!

## statement 5
- You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.

## statement 5
<img src="img/classic_NO.jpg" width=400 height=400 />

## statement 5
- You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.
- ummm, isn't that $\alpha$?
- **classic** NO: $\alpha$ is about an infinite amount of experiments
- in this case, the probability is $1 - p(H_0)$
- cannot get $p(H_0)$ without Bayes, so this statement is wrong


## statement 6
- You have a reliable experimental finding in the sense that, if the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.

## statement 6
<img src="img/unsure_NO.jpg" width=400 height=400 />

## statement 6
- You have a reliable experimental finding in the sense that, if the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.
- that's the **replication fallacy** [@haller2002misinterpretations]

## how often do people get it wrong?
<img src="img/haller_misinterpretation.png" width=600 height=500 />

# frequentist inference <!-- Fabian -->

## how a frequentist draws inference
- suppose you are being asked 26 questions about Bayesian statistics ($N = 26$)
- you happen to answer 8 correctly ($k = 8$)
- are you better than guessing?
- test $H_0: \theta = \frac{1}{2}$ against $H_1: \theta \neq \frac{1}{2}$
  
## how a frequentist draws inference
- to obtain the **sampling distribution** assume that $H_0: \theta = \frac{1}{2}$ holds
- imagine that the experiment is repeated very many times under **identical circumstances**
- for each **hypothetical** experiment, calculate the test statistic of the **hypothetical** sample
- the distribution of these test statistics are called the **sampling distribution**

## graphically [@wagenmakers2007practical]
<img src="img/sampling_distribution.png" />

## sampling plan
- there are two different ways to conduct the experiment
- either fix $N = 26$ and count the number of correct answers
- or ask so many questions until one gets $k = 8$ correct answers
- first one is binomial, the second one is negative binomial
- $p(k | \theta, N) = \binom{n}{k} \theta^k (1 - \theta)^{n - k}$
- $p(N | \theta, z) = \binom{n - 1}{k - 1} \theta^k (1 - \theta)^{n - k}$

## note
- a frequentist **needs** this sampling distribution
- often, properties of the sampling distribution can be derived analytically from the sample data
- for example, the variance of the **sampling distribution** of the **sample mean** is $\sigma^2 = \frac{s^2}{N}$
- this is one reason why frequentism got so much traction: it is computationally trivial

## bootstrapping
- in order to **see** what happens here, let us simulate a large number of replications of the experiment!
- we will draw independent random samples from our actual observed data, and compute the test statistic of that draw
- in the binomial case, the test statistic is the **number of correct answers**
- in the negative binomial case, the test statistic is the **number of questions**

## binomial bootstrapping
```{r}
binom.boot <- function(dat, samples = 10000) {
    n <- length(dat) # is fixed!
    result <- numeric(samples)

    for (i in 1:samples) {
        draw <- sample(dat, n, replace = TRUE)
        result[i] <- sum(draw)
    }

    result
}
```

## negative binomial bootstrapping
```{r}
nbinom.boot <- function(dat, samples = 10000) {
    k <- sum(dat) # is fixed!
    result <- numeric(samples)

    for (i in 1:samples) {
        ss <- sample(c(0, 1), 1, prob = c(0.5, 0.5))

        while (sum(ss) != k) {
            ss <- c(ss, sample(c(0, 1), 1, prob = c(0.5, 0.5)))
        }

        result[i] <- length(ss)
    }
    result
}
```

## practical difference
<img src="img/sampling_binomial.png" height="500" width="700"/>

## frequentist inference
- assumes that the parameter $\theta$ is fixed
- only the data is allowed to vary
- we can intuit that confidence intervals, statistical power etc. are not properties of the data
- they are properties of the **testing procedure**


# p-values: a death in five acts <!-- Fabian -->

## act I <!-- Fabian -->
<img src="img/sampling_distribution.png" width=700 height=500 />


## act I <!-- Fabian -->
- result of statistical analysis depends on the intention of the researcher
- because those intentions define the space of all possible (unobserved) data [@wagenmakers2007practical]


## act I
- suppose I ask you 26 questions on Bayesian stats, and you get 8 right
- the last question you answered, you answered correctly
- were you better than chance? $H_0: \theta = 0.5$
- there are two possible sampling plans
- ask 26 questions, and see how many you answered correctly (**binomial**)
- ask questions so long until you answer 8 correctly (**negative binomial**)
- ***the data are the data are the data*** (example from Kruschke, 2010, Ch. 11)

## act I
<img src="img/sampling_binomial.png" width=700 height=500 />

## act I
```{r}
N <- 26
k <- 8

binomial <- pbinom(k, N, 0.5) * 2
negbinomial <- pnbinom(k, N, 0.5) * 2

binomial # fail to reject

negbinomial # science paper!!
```


## act II <!-- Fabian -->
- optional stopping [@sanborn2014frequentist; @rouder2014optional]
- look at data, test, gather more data, test, etc.

## act II, demo
```{r, message = FALSE, eval = FALSE}
source("http://rynesherman.com/phack.r") # read in the p-hack function

res <- phack(initialN=30, hackrate=5, grp1M=0, grp2M=0, grp1SD=1, grp2SD=1,
   maxN=200, alpha=.05, alternative="two.sided", graph=TRUE, sims=2000)
```

## act II, demo
<img src="img/phack1.png" />

## act II, implications
- inflated $\alpha$ (p-hacking!)
- assume you tested 20 participants, and get $p = 0.057$
- unethical
- money & ressources
- just downright horrible

## act II, Bayes note
> "the rules governing when data collection stops are irrelevant to data interpretation. It
> is entirely appropriate to collect data **until a point has been proven or disproven**, or until
> the data collector runs out of time, money, or patience."

- Edwards et al. (1963)


## act III <!-- Fabian -->
- don't quantify statistical evidence; @wagenmakers2007practical
- $p = 0.04, n = 10$ is more evidence than $p = 0.04, n = 1000$
- in fact, the latter is support for $H_0$!
- are violently biased against $H_0$

## act IV <!-- Fabian -->
- violently biased against $H_0$
- underlying logic of p-values: either the null hypothesis is false, or a rare event has occured
- following example taken from Rouder et al. (in press)

## p-values: flawed reasoning
- the following is logically correct:
- (Premise): If Hypothesis $H$ is true, then event $X$ will not occur.
- (Premise): Event $X$ occured.
- (Conclusion): Hypothesis $H$ is not true.

## p-values: flawed reasoning
- this **does not** translate to probabilistic settings
- (Premise): If Hypothesis $H$ is true, then event $X$ is **unlikely**.
- (Premise): Event $X$ occured.
- (Conclusion): Hypothesis $H$ is **probably not true**.

## p-values: flawed reasoning
- take this example as demonstration:
- (Premise): If Jane is an American, then it will be unlikely that she is a U. S. Congressperson.
- (Premise): Jane is a U. S. Congressperson.
- (Conclusion): Jane is probably not an American.
- by only looking at $p(D|H_0)$, p-values are violently biased against $H_0$

## the case of Sally Clark
- both Clark's babies died, where $p(\text{baby dies}) = \frac{1}{8543}$
- thus the probability that both babies died is roughly 1 in 73 million
- since this is soo incredibly low ($p < 0.00001$ or whatnot), Clark **probably killed** her babies
- indeed, in November 1999, a jury found poor Sally guilty of double murder

## see any problem with that?

## the case of Sally Clark
> "The jury needs to weigh up two competing explanations for the babies’ deaths: SIDS or murder. The fact that two deaths by SIDS is
> quite unlikely is, taken alone, of little value. Two deaths by murder may well be even
> more unlikely. What matters is the relative likelihood of the deaths under each
> explanation, not just how unlikely they are under one explanation."

- **President of the Royal Statistical Society** (2002)

## the case of Sally Clark
- in fact, $p(\text{baby dies} | \text{sudden infant death})$ is higher than $p(\text{baby dies} | \text{murder})$
- the ratio of these two - the **likelihood ratio** - is the proper measure of statistical evidence
- statistical evidence is **always** relative; there is **no free lunch**


## act V <!-- Fabian -->
- p-value give you $p(\text{D or more extreme}|H_0)$
- what we want is $p(H|D)$, the probability that our hypothesis is true!
- there is a subtle, but important difference
- $p(\text{you are dead} | \text{shark has bitten off your head})$ is very high :(
- $p(\text{shark has bitten off your head} | \text{you are dead})$ is very low!


## Effect Sizes and Confidence Intervals: The *Frequentist* Solution?
- Effect Size
    * A measure of the strength of an effect
    * e.g., a correlation of *r* = .35, a difference between experimental conditions of Cohen's <em>d</em> = 0.30
    
- Confidence interval
    * The area within which in 95% of replications (same experiment/measures, same sample size drawn from the same population) the real parameter (effect) will be

- Example:
    * *There was a significant difference between the control group and the intervention group, p = .002, d = 0.34 [0.22, 0.44]*

## Effect Sizes and Confidence Intervals: A Solution?

<div align="center">
<img height='400' src='img/cummingcover.png'>
</div>

## Cumming, 2014
* *"We need to make substantial changes to how we conduct research"*

## Cumming, 2014
- Replicate (x)
- Adapt meta-analytical thinking (x)
- Avoid NHST (x)
- Don't trust any p-value (x)

## Cumming, 2014

<div align="center">
<img height='500' src='img/cumming_25table.png'>
</div>

## Cumming, 2014

- ES are the main research outcome (...?)
- The CI tells us the precision of a study (...?)
  	* ...a much better approach than declaring the result "statistically significant" (...?)
- "Enjoy the benefits!" (...?)

## Cumming, 2014

* So, what does a SINGLE confidence interval tell me?
* Nothing

## ES and CI are NOT the solution
### Part I: Morey et al., 2014

* "For psychological science to be a healthy science, both estimation and hypothesis testing are needed."
* Estimation is necessary in pretheoretical work before clear predictions can be made, and for theory revision.
* Hypothesis testing, not estimation, is necessary for testing the quantitative predictions of theories.
* None is more informative than the other
* They answer different questions.
* Estimation alone produces a massive catalogue devoid of theoretical content
* Hypothesis testing alone may cause researchers to miss rich, meaningful structure in data.
* It is crucial for estimation and hypothesis testing to be advocated side by side

## ES and CI are NOT the solution
### Part II: Hoekstra et al., 2014

<div align="center">
<img height='400' src='img/hoekstra_CIquestions.png'>
</div>

## ES and CI are NOT the solution
### Part II: Hoekstra et al., 2014

<div align="center">
<img height='400' src='img/hoekstra_results.png'>
</div>

## ES and CI are NOT the solution
### Part III: Lee, 2014

(Lee, [here](https://webfiles.uci.edu/mdlee/Lee2014_NewStatistics.pdf))

<div align="center">
<img height='400' src='img/michaellee.png'>
</div>

## recap
- p-values are subjective
- p-values are biased against $H_0$
- p-values don't quantify statistical evidence
- p-values address the wrong question
- confidence intervals are equally flawed
- inference based on p-values cannot support $H_0$
- but invariances are of theoretical importance!

# Likelihoods

## likelihoods
- in conditional probability, say $p(D|\theta)$, the hypothesis is fixed and the data vary
- in likelihoods, $L(\theta|D)$, the data is fixed and the hypothesis varies
- $L(\theta|D) \propto K \times p(D|\theta)$, need not sum to one

## likelihoods
- **law of likelihood**: data supports $H_0$ **over** $H_1$ iff $p(D|H_0) > p(D|H_1)$
- **likelihood principle**: all information relevant for statistical evidence is contained in the likelihood function
- for both the binomial and the negative binomial case, the likelihood is $\theta^k (1 - \theta)^{n-k}$
- intentions do not matter
- and there is no issue with optional stopping!
- for more, see @royall1997statistical

## likelihood function
- is just the likelihood, $L(\theta|D)$, plotted for every value of $\theta$
- Shiny demo!
```{r, eval = FALSE}
library('shiny')

setwd('~/code/bayes_workshop/code')
runApp('likelihood')
```

## likelihood ratio
- make terrible sense, right?
- derive Bayes' rule (pinboard)
- are the factor by which we update our prior beliefs to obtain posterior beliefs
- $\frac{p(H_0)}{p(H_1)}$
- $\frac{p(H_0|D)}{p(H_1|D)}$
- $\frac{p(D|H_0)}{p(D|H_1)}$
- thus: $\frac{p(H_0|D)}{p(H_1|D)} = \frac{p(D|H_0)}{p(D|H_1)} \times \frac{p(H_0)}{p(H_1)}$

## likelihood ratio
- however, we often don't know the point value of the alternative (true effect size)
- we can quantify our uncertainty with a **prior probability distribution**
- write stuff on the pinboard

## note on priors
> "Today one wonders how it is possible that orthodox logic continues to be taught in some places year after year and praised as 'objective', while Bayesians are charged with 'subjectivity'.
Orthodoxians, preoccupied with fantasies about nonexistent data sets and, in principle, unobservable limiting frequencies - while ignoring relevant prior information - are in no position to charge
anybody with 'subjectivity'."

- Jaynes (2003), as cited in [@lee2014bayesian, p.61]

# Bayesian Inference <!-- Fabian -->

## prior distributions
- $\frac{p(H_0|D)}{p(H_1|D)} = \frac{p(D|H_0)}{p(D|H_1)} \times \frac{p(H_0)}{p(H_1)}$
- $\text{posterior odds} = \text{Bayes factor} \times {\text{prior odds}}$
- important distinction: prior on parameters, prior on models (BEM)

## Bayesian inference
- [demo!](http://87.106.45.173:3838/felix/BayesLessons/BayesianLesson1.Rmd)

## Bayes factors <!-- Fabian -->
- computationally hard
- Savage-Dickey! [@wagenmakers2010bayesian]
<img src="img/sdratio_plot.png" height="500" />

## Interpretation 
<img src="img/BF_scale.png" height=400 />
- from @vandekerckhove2014model

## Credibility Intervals <!-- Fabian -->
- **real** confidence intervals
- you can be 95% confident that the parameter lies within this range!

## solutions <!-- Fabian -->
- tackle the frequentist problems
- conceptualize probability as degree of belief
- likelihood principle!

## *Why aren't we all Bayesians?* <!-- Peter -->
- "Why can't we all just be Bayesian?" (Lee, [here](https://webfiles.uci.edu/mdlee/Lee2014_NewStatistics.pdf))
- we can now!
- Computationally intensive
    * Nowadays, no problem with MCMC
- Subjectivity of priors
    * A "good" thing or a "bad" thing?
- What "type of model" is this?
    * Exactly the same as the models you know!
- Software
    * R: BUGS/JAGS
    * JASP


# practical Bayes

## JASP
- what is the time?
- is developed by the lab of EJ Wagenmakers
- lead developer is Jonathan Love


## JASP
- various data sets
- Wagenmakers et al. are advocates of pre-registration, confirmatory research, direct replications
- in the remaining time we will analysis data sets from some replications

## turning the hands of time
- counter clockwise movements lead to an orientation towards the future and novelty (Topolski and Sparenberg, 2012)
- @wagenmakers2015turning directly replicated this research

## turning the hands of time
<img src="img/turning_hands.png" height="500" width="700"/>

## preparation
- open JASP and load the data file called **kitchen rolls**
- look at the descriptives, plot the data

## freq analysis
- run a classical t-test, with rotation as independent and mean NEO as dependent variable
<img src="../code/exercises/kitchenfreq_ttest.png" />

## freq analysis
- what does this p-value mean?
- what does this confidence interval mean?
- is there a difference between the groups?
- did we replicate the finding?
- well, we *cannot* support $H_0$ using frequentist statistics
- let's be Bayesian!

## Bayesian analysis
- the model, a t-test, is the same
- $BF_{01} = \frac{1}{BF_{01}}$
<img src="../code/exercises/kitchenbayes_ttest1.png" />

## Bayesian analysis
- note that we expect the effect to be directional; it would be wrong to test two-sided
- change the **hypothesis** to be *Group one > Group two*
- how does the Bayes factor change?
- note that here we specify a prior over the effect size $\theta \sim \text{Cauchy(scale=0.707)}$
- in the pre-registration, Wagenmakers and colleagues said that they would use a scale of 1
- change the width of the Cauchy under the section **prior**
- what do you observe? what happens when you set the scale to 2000000000?
- diffuse priors should not be used for hypothesis testing [@lindley1957statistical; @degroot1982lindley]

## Bayesian analysis
<img src="../code/exercises/kitchenbayes_ttest2.png" />

## Bayesian analysis
- we see that the Bayes factor changes; how robust is it?
- run a sensitivity analysis, i.e. **a Bayes factor robustness check**
<img src="../code/exercises/kitchenbayes_robust.png" />


## Bayesian analysis
- Bayes did not only allow us to quantify support for $H_0$ in a continuous fashion
- it also allowed us to be incredibly flexible in data collection
- run a **sequential analysis** to see this
<img src="../code/exercises/kitchen_sensitivity.png" />

## kitchen rolls wrap up
- you can find the pre-registration form of this research [here](https://osf.io/p3isc/), see the section **sampling plan**
- they failed to replicate the effect; the data were about 10 times more likely under $H_0$ then under $H_1$
- descriptives point in the other direction as originally observed (higher openness when counter clock wise)

## bathing habits
- lonely people compensate the lack of social warmth by taking warmer showers and baths [@bargh2012bath]
- ummmmmmm, really?
- @donnellan2014association replicated this in 9 experiments with over 3000 participants
- @wagenmakers2015absence re-analysed the data using Bayesian inference
- found that three out of those nine experiments yielded uninformative data (1/3 < BF < 3), although they had high power!
- data from [here](https://sites.google.com/site/erasmusmathpsych/2015-seminar/materials/bayesian-inference)

## analysis
- analyse bathing data from experiments 2, 6, and 9
- first plot the descriptives, then run a classical correlation, then a Bayesian one
- what do you observe?
- for Bayesian inference on correlations, which prior would you choose?
- the following describe directed tests (positive correlation)

## analysis experiment 2
- very high p-value - what can we say?
- BF says there is strong evidence for no correlation
<img src="../code/exercises/bathing2.png" />

## analysis experiment 6
- significant p-value - what can we say?
- BF says there is uninformative data!
<img src="../code/exercises/bathing6.png" />

## analysis experiment 9
- p-value is high - what can we say?
- BF says there is strong evidence for no correlation
<img src="../code/exercises/bathing9.png" />

## bathing wrap up
- p-values are uniformly distributed under $H_0$
- must be Bayesian to quantify support for $H_0$ 
- because there is a difference between $p > 0.05$, the data are uninformative
- and $p > 0.05$, data are informative and support $H_0$
- classical statistics **does not make** this distinction (for more, see [this](http://goo.gl/p0N1QE))
- run the tests using undirected hypotheses!

## there are moar data sets!
- you can find them in the **/data** directory
- check them out on your own :)

## a power fallacy
- "Power is a pre-experimental concept that averages over all possible outcomes of an experiment, only one of which is actually observed."
- urn 0: 9 green balls and 1 blue ball
- urn 1: 9 green balls and 1 orange ball
- draw one ball (power = 10%); but it's orange! highly informative
- draw nine balls (power = 90%); but all green! uninformative
- high power **does not** imply **diagnostic** data! [@wagenmakers2014power]

# Wrap up <!-- Fabian -->

# We should all just be Bayesian <!-- Fabian -->
<!--  "In 20 years, our children will ask us
      why we've ever been frequentist,
      and the answer will be difficult"
-->

## References